{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 6 - movie poster classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label (meaning that there is more than one \"true\" label) classification of movie poster images by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from skimage import io\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We use a PyTorch `Dataset` to represent the data, which means we must implement `__init__`, `__len__` and `__getitem__`.  For efficiency's sake, we ideally want to load the image data (movie posters and their genre classifications) and represent them as a `Tensor` in memory.\n",
    "\n",
    "The movie posters had to be converted to images of the same size and colour channels.  The resizing can be done inside Python but is slow, so they were converted on-disk using command-line tools. The colour channels are more efficient than resizing in Python, so that was done in Python.\n",
    "\n",
    "Note that we need to permute the dimensions of the `Tensor` we create from `skimage` NumPy arrays. The latter represent the colour channels (three, for red-green-blue) as the innermost dimension (the fourth dimension, or dimension 3, meaning that pixels are represented as an array of three colour values).  The convolutional layers in PyTorch require the colour channel to be the second dimension (after the batch dimension, meaning that the image is represented as three overlapping images, one for each colour, where each pixel is a single value).  You need to use `Tensor.permute` not `Tensor.view`---the latter just redraws the \"boxes\" inside the array, it doesn't rearrange the data for a different order of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 23\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from skimage import transform\n",
    "from skimage import color\n",
    "\n",
    "class MoviePosterDataset(Dataset):\n",
    "    def __init__(self, csvfile, imagedir, device=device):\n",
    "        self.posterlist = pd.read_csv(csvfile)\n",
    "        self.imagedir = imagedir\n",
    "        \n",
    "        imageids = list(self.posterlist[\"Id\"])\n",
    "        imagefiles = [\"{}/{}.jpg\".format(self.imagedir, x) for x in imageids]\n",
    "        images = [np.array(io.imread(x)) for x in imagefiles]\n",
    "        images = np.array([color.gray2rgb(x) if len(x.shape) < 3 else x for x in images])\n",
    "        \n",
    "        truths = self.posterlist[self.posterlist.columns[2:]]\n",
    "        self.truths = torch.Tensor(truths.to_numpy())\n",
    "    \n",
    "        tns = torch.from_numpy(images)\n",
    "        self.images = tns.permute(0, 3, 1, 2)\n",
    "        \n",
    "        if device != \"cpu\":\n",
    "            self.device = torch.device(device)\n",
    "            self.images = self.images.to(self.device)\n",
    "            self.truths = self.truths.to(self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.posterlist)\n",
    "    \n",
    "    def __getitem__(self, idx):            \n",
    "        truths = self.truths[idx]\n",
    "        images = self.images[idx]\n",
    "        \n",
    "        return images, truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the `Tensor` on the CPU because of memory limitations created by sticking to one GPU (which only has 10GB of space to itself).  We will move it to the GPU in batches instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading the dataset.\n"
     ]
    }
   ],
   "source": [
    "mpd = MoviePosterDataset(\"Multi_Label_dataset/train.csv\", \n",
    "                         \"Multi_Label_dataset/ImageSmaller\", device=\"cpu\")\n",
    "print(\"Finished loading the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7254 poster images and all the images have been converted to a shape of (3, 300, 450). There are 25 movie labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7254, 3, 300, 450]), torch.Size([7254, 25]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd.images.shape, mpd.truths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "Defining a model with convolutional layers for images is technically a lot easier than defining an RNN-based model for human language.  The `Conv2d` layer automatically moves a 5x5 filter across the entire image, no effort required to manage padding and packing and unpacking and sequence issues. We do have to flatten the output of the `MaxPool2d` layer to feed it to the subsequent `Linear` layers.  The output of the model comes from `Sigmoid` over the number of classes, so that we have a binary classification for the 25 separate movie labels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model parameters\n",
    "\n",
    "X = Batch size\n",
    "\n",
    "Layer 1\n",
    "* First convolutional layer: Size changes from `[X, 3, 300, 450]` to `[X, 16, 300, 450]`.\n",
    "* Dropout: 0.8\n",
    "* First pooling layer: Size changes from `[X, 16, 300, 450]` to `[X, 16, 150, 225]`.\n",
    "\n",
    "Layer 2\n",
    "* Second convolutional layer: Size changes from `[X, 16, 150, 225]` to `[X, 32, 150, 225]`.\n",
    "* Dropout: 0.25\n",
    "* Second pooling layer: `[X, 32, 75, 112]` to `[X, 32, 75, 112]`.\n",
    "\n",
    "Layer 3\n",
    "* Third convolutional layer: Size changes from `[X, 32, 75, 112]` to `[X, 64, 75, 112]`.\n",
    "* Dropout: 0.25\n",
    "* Third pooling layer: Size changes from `[X, 64, 75, 112]` to `[X, 64, 37, 56]`.\n",
    "\n",
    "\n",
    "After training the model using 3 layers, the model starts to overfit; it's learning to recognize specific images in the training set rather than generalizing. Adding dropout to all the layers did not help to avoid overfitting. \n",
    "\n",
    "The  model with the lowest loss was trained using 3 layers, a dropout of 0.8 at the first layer and a dropout of 0.25 for the others. However, the model starts to overfit after 5 epochs; the validation keeps increasing while the training loss stays close to 0.\n",
    "\n",
    "![pl1](/img/3l_lr0001.png \"Plot 1\")\n",
    "3 layers, dropout 0.8 for input layer, 0.25 for the rest, learning rate: 0.0001\n",
    "\n",
    "Using the same dropout, but adding a 4th layer produces similar results as only using 3 layers, but the model takes longer (10 epochs) before it starts to overfit. The model with 3 layers performed slightly better so I decided to keep it like this.\n",
    "![pl2](/img/4l_lr0001.png \"Plot 2\")\n",
    "4 layers, dropout 0.8 for input layer, 0.25 for the rest, learning rate: 0.0001\n",
    "\n",
    "Training the model using 4 layers with a higher learning rate gives a higher loss than the 3 layers and after 20 epochs, both the training and validation loss increases. Training the model with a lower learning rate, even when including dropout for all layers, causes the model to overfit.\n",
    "\n",
    "![pl3](/img/4l_lr001.png \"Plot 3\")\n",
    "4 layers, dropout 0.8 for input layer, 0.25 for the rest, learning rate: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosterClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "    \n",
    "        # Kernel Size: the size of the filter.\n",
    "        \n",
    "        # Stride: the rate at which the kernel passes over the input image. \n",
    "        # A stride of 2 moves the kernel in 2-pixel increments.\n",
    "        \n",
    "        # Padding : add layers of 0s to the outside of the image \n",
    "        # in order to make sure that the kernel properly passes over the edges of the image.\n",
    "        # Padding should be smaller than half the kernel size\n",
    "        \n",
    "        self.input_dropout = nn.Dropout(0.8)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "        # Layer 1\n",
    "        self.conv1  = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5, stride=1, padding=2)\n",
    "        self.bnm1 = nn.BatchNorm2d(16)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bnm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bnm3 = nn.BatchNorm2d(64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        # Layer 4\n",
    "        self.conv4 = nn.Conv2d(in_channels=64,out_channels=128, kernel_size=5, stride=1, padding=2)\n",
    "        self.bnm4 = nn.BatchNorm2d(128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear0 = nn.Linear((64*37*56), 3000)\n",
    "        #self.linear0 = nn.Linear(128 * 18 * 28, 3000)\n",
    "        self.tanh = nn.Tanh()    \n",
    "        self.linear1 = nn.Linear(3000, 25) # 25 output classes\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # conv -> relu -> maxpool        \n",
    "        # Computes the activation of the first convolution\n",
    "        # X is batch size\n",
    "        \n",
    "        #Layer 1\n",
    "        #Size changes from [X, 3, 300, 450] to [X, 16, 300, 450]\n",
    "        output = self.conv1(input)\n",
    "        output = self.bnm1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.input_dropout(output) \n",
    "        #print(\"Conv1\", output.size())\n",
    "        #Size changes from [X, 16, 300, 450] to [X, 16, 150, 225]\n",
    "        output = self.maxpool1(output)\n",
    "        #print(\"Pool1\", output.size())\n",
    "        \n",
    "        #Layer 2\n",
    "        #Size changes from [X, 16, 150, 225] to [X, 32, 150, 225]\n",
    "        output = self.conv2(output)\n",
    "        output = self.bnm2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)   \n",
    "        #print(\"Conv2\", output.size())\n",
    "        #Size changes from [X, 32, 75, 112] to [X, 32, 75, 112]\n",
    "        output = self.maxpool2(output)\n",
    "        #print(\"Pool2\", output.size())\n",
    "        \n",
    "        #Layer 3\n",
    "        #Size changes from [X, 32, 75, 112] to [X, 64, 75, 112]\n",
    "        output = self.conv3(output)\n",
    "        output = self.bnm3(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output) \n",
    "        #print(\"Conv3\", output.size())\n",
    "        #Size changes from [X, 64, 75, 112) to [X, 64, 37, 56]\n",
    "        output = self.maxpool3(output)\n",
    "        #print(\"Pool3\", output.size())\n",
    "        \n",
    "        #Layer 4\n",
    "        #Size changes from [X, 64, 75, 112] to [X, 128, 75, 112]\n",
    "        #output = self.conv4(output)\n",
    "        #output = self.bnm4(output)\n",
    "        #output = self.relu(output)\n",
    "        #output = self.dropout(output) \n",
    "        #print(\"Conv4\", output.size())\n",
    "        #Size changes from [X, 128, 75, 112] to [X, 128, 18, 28]\n",
    "        #output = self.maxpool4(output)\n",
    "        #print(\"Pool4\", output.size())\n",
    "        \n",
    "        #output = output.view(-1, 3*90*60)\n",
    "        output = output.view(-1, 64*37*56) # output size of final pooling layer\n",
    "        #output = output.view(-1, 128 * 18 * 28)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear0(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.linear1(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange the data for the model\n",
    "\n",
    "We're going to do a 60/40 train/test split using PyTorch's own samplers and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7254"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalindices = list(range(len(mpd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.shuffle(totalindices)\n",
    "splitindex = math.floor(len(mpd)*0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4352"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitindex"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# sample set used for testing part of the dataset\n",
    "# sample = len(mpd)//4\n",
    "# totalindices = list(range((sample)))\n",
    "# random.shuffle(totalindices)\n",
    "# splitindex = math.floor((sample)*0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part #1: validation data (4 points)\n",
    "\n",
    "Adjust the code in the notebook to give a 60/20/20 train/validation/testing split of the data. Testing data is split 50/50 into test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingindices = totalindices[:splitindex]\n",
    "testingindices_ = totalindices[splitindex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitindex2 = math.floor(len(testingindices_)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitindex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingindices = testingindices_[:splitindex2]\n",
    "validationindices = testingindices_[splitindex2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingsampler = torch.utils.data.SubsetRandomSampler(trainingindices)\n",
    "testingsampler = torch.utils.data.SubsetRandomSampler(testingindices)\n",
    "validationsampler = torch.utils.data.SubsetRandomSampler(validationindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4352\n",
      "Number of validation examples: 1451\n",
      "Number of testing examples: 1451\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(trainingsampler)}')\n",
    "print(f'Number of validation examples: {len(validationsampler)}')\n",
    "print(f'Number of testing examples: {len(testingsampler)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl = torch.utils.data.DataLoader(mpd, batch_size=batches, \n",
    "                                      sampler=trainingsampler, pin_memory=False)\n",
    "valdl = torch.utils.data.DataLoader(mpd, batch_size=batches, sampler=validationsampler, pin_memory=False)\n",
    "testdl = torch.utils.data.DataLoader(mpd, sampler=testingsampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jackard Index\n",
    "The size of the intersection divided by the size of the union of two label sets:\n",
    "A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "\n",
    "CNN for image identification applications apply the Jaccard Index measurements as a way of conceptualizing accuracy of object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(output, truth):\n",
    "    jaccard_loss = 0.0\n",
    "    for i in range(len(truth)):\n",
    "            tru = [a.item() for a in truth[i]]\n",
    "            out = [1.0 if index.item() > 0.5 else 0.0 for index in output[i]]\n",
    "            intersection = 0.0\n",
    "            union = 0.0\n",
    "            for i in range(len(tru)):\n",
    "                if tru[i] == out[i]:\n",
    "                    intersection += 1\n",
    "                union += 1\n",
    "            jaccard_loss += float(intersection)/float(union)\n",
    "    return jaccard_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write and run the training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"image_model\"\n",
    "model = PosterClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosterClassifier(\n",
       "  (input_dropout): Dropout(p=0.8, inplace=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bnm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bnm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bnm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bnm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (linear0): Linear(in_features=132608, out_features=3000, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (linear1): Linear(in_features=3000, out_features=25, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 398,172,745 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "The model computes the loss for the training set and the validation set and saves the model with the lowest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate= 0.0001\n",
    "decay=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def train(train_dataloader, val_dataloader, model,epochs=3):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    criterion = criterion.to(device)\n",
    "    best_val_loss = 0.0\n",
    "    jaccard_losses = []\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        train_loss = 0.0        \n",
    "        train_batches = 0\n",
    "        \n",
    "        for c, data in enumerate(train_dataloader):\n",
    "            images, truth = data\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images.float().to(device))\n",
    "            loss = criterion(output, truth.to(device))\n",
    "            train_loss += loss\n",
    "            train_batches += 1.0\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0        \n",
    "        val_batches = 0        \n",
    "        jaccard_loss = 0.0\n",
    "        \n",
    "        model.eval()\n",
    "        for c, data in enumerate(val_dataloader):\n",
    "            images, truth = data\n",
    "            output = model(images.float().to(device))\n",
    "            with torch.no_grad():  \n",
    "                val_loss += criterion(output, truth.to(device))          \n",
    "            val_batches += 1.0   \n",
    "            jaccard_loss += jaccard(output, truth.to(device)) \n",
    "            \n",
    "        validation_loss = val_loss/val_batches\n",
    "        validation_loss = validation_loss.item()       \n",
    "        training_loss = train_loss/train_batches\n",
    "        training_loss = training_loss.item()\n",
    "        jaccard_loss = jaccard_loss/float(batches*val_batches)\n",
    "           \n",
    "        val_losses.append(validation_loss)\n",
    "        jaccard_losses.append(jaccard_loss)\n",
    "        train_losses.append(training_loss)       \n",
    "        \n",
    "        # save best model\n",
    "        if epoch == 0:\n",
    "            best_val_loss = validation_loss        \n",
    "        if validation_loss < best_val_loss:\n",
    "            best_val_loss = validation_loss\n",
    "            print(\"Best loss so far!\")\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            #torch.save(model, model_name)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)  \n",
    "               \n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {training_loss:.3f}')\n",
    "        print(f'\\t Validation Loss: {validation_loss:.3f}')\n",
    "        print(f'\\t Lowest Loss: {best_val_loss:.3f}')\n",
    "        print(f'\\t Jaccard Index: {jaccard_loss:.3f}')\n",
    "        \n",
    "    return val_losses, train_losses, jaccard_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 33s\n",
      "\tTrain Loss: 0.481\n",
      "\t Validation Loss: 0.247\n",
      "\t Lowest Loss: 0.247\n",
      "\t Jaccard Index: 0.895\n",
      "Best loss so far!\n",
      "Epoch: 02 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.246\n",
      "\t Validation Loss: 0.239\n",
      "\t Lowest Loss: 0.239\n",
      "\t Jaccard Index: 0.896\n",
      "Best loss so far!\n",
      "Epoch: 03 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.235\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 0.898\n",
      "Best loss so far!\n",
      "Epoch: 04 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.237\n",
      "\t Validation Loss: 0.234\n",
      "\t Lowest Loss: 0.234\n",
      "\t Jaccard Index: 0.899\n",
      "Epoch: 05 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.233\n",
      "\t Validation Loss: 0.236\n",
      "\t Lowest Loss: 0.234\n",
      "\t Jaccard Index: 0.899\n",
      "Best loss so far!\n",
      "Epoch: 06 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.225\n",
      "\t Validation Loss: 0.232\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.901\n",
      "Epoch: 07 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.208\n",
      "\t Validation Loss: 0.235\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.900\n",
      "Epoch: 08 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.171\n",
      "\t Validation Loss: 0.242\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.897\n",
      "Epoch: 09 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.121\n",
      "\t Validation Loss: 0.255\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.897\n",
      "Epoch: 10 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.075\n",
      "\t Validation Loss: 0.273\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.897\n",
      "Epoch: 11 | Epoch Time: 0m 33s\n",
      "\tTrain Loss: 0.045\n",
      "\t Validation Loss: 0.291\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.896\n",
      "Epoch: 12 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.027\n",
      "\t Validation Loss: 0.316\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.896\n",
      "Epoch: 13 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.017\n",
      "\t Validation Loss: 0.337\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 14 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.012\n",
      "\t Validation Loss: 0.353\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.895\n",
      "Epoch: 15 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.008\n",
      "\t Validation Loss: 0.368\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 16 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.006\n",
      "\t Validation Loss: 0.380\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 17 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.005\n",
      "\t Validation Loss: 0.394\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.895\n",
      "Epoch: 18 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.004\n",
      "\t Validation Loss: 0.405\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.895\n",
      "Epoch: 19 | Epoch Time: 0m 33s\n",
      "\tTrain Loss: 0.003\n",
      "\t Validation Loss: 0.416\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 20 | Epoch Time: 0m 33s\n",
      "\tTrain Loss: 0.003\n",
      "\t Validation Loss: 0.423\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 21 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.002\n",
      "\t Validation Loss: 0.435\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 22 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.002\n",
      "\t Validation Loss: 0.441\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 23 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.002\n",
      "\t Validation Loss: 0.447\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 24 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.001\n",
      "\t Validation Loss: 0.457\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 25 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.001\n",
      "\t Validation Loss: 0.463\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 26 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.001\n",
      "\t Validation Loss: 0.467\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 27 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.001\n",
      "\t Validation Loss: 0.473\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 28 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.001\n",
      "\t Validation Loss: 0.479\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 29 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.001\n",
      "\t Validation Loss: 0.485\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n",
      "Epoch: 30 | Epoch Time: 0m 32s\n",
      "\tTrain Loss: 0.001\n",
      "\t Validation Loss: 0.489\n",
      "\t Lowest Loss: 0.232\n",
      "\t Jaccard Index: 0.894\n"
     ]
    }
   ],
   "source": [
    "val_losses, train_losses, jaccard_losses = train(traindl,valdl, model,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(val_losses, label=\"Validation\")\n",
    "#plt.plot(jaccard_losses, label='Jaccard',linestyle='--')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "For memory purposes, we're keeping the testing data on the CPU memory still.  We have to put the model into evaluation mode with `model.eval`, which turns off the dropout and other regularization useful in training---we want the test to represent a deterministic result of the trained model. We also test a single epoch as one big batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trained = torch.load(model_name)\n",
    "device = \"cpu\" # cuda out of memory\n",
    "model = model.to(device) \n",
    "model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    sumloss = 0.0\n",
    "    items = 0.0\n",
    "    jaccard_loss = 0.0\n",
    "    \n",
    "    for c, data in enumerate(dataloader):\n",
    "        images, truth = data\n",
    "        output = model(images.float().to(device))\n",
    "        sumloss += criterion(output, truth.to(device))\n",
    "        jaccard_loss += jaccard(output, truth.to(device))\n",
    "        items += 1.0\n",
    "        \n",
    "    print(f'\\t Test Loss: {sumloss/items:.3f}')\n",
    "    print(f'\\t Jaccard Index: {jaccard_loss/items:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Test Loss: 0.236\n",
      "\t Jaccard Index: 0.913\n"
     ]
    }
   ],
   "source": [
    "test(model, testdl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
