{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 6 - movie poster classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label (meaning that there is more than one \"true\" label) classification of movie poster images by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from skimage import io\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We use a PyTorch `Dataset` to represent the data, which means we must implement `__init__`, `__len__` and `__getitem__`.  For efficiency's sake, we ideally want to load the image data (movie posters and their genre classifications) and represent them as a `Tensor` in memory.\n",
    "\n",
    "The movie posters had to be converted to images of the same size and colour channels.  The resizing can be done inside Python but is slow, so they were converted on-disk using command-line tools. The colour channels are more efficient than resizing in Python, so that was done in Python.\n",
    "\n",
    "Note that we need to permute the dimensions of the `Tensor` we create from `skimage` NumPy arrays. The latter represent the colour channels (three, for red-green-blue) as the innermost dimension (the fourth dimension, or dimension 3, meaning that pixels are represented as an array of three colour values).  The convolutional layers in PyTorch require the colour channel to be the second dimension (after the batch dimension, meaning that the image is represented as three overlapping images, one for each colour, where each pixel is a single value).  You need to use `Tensor.permute` not `Tensor.view`---the latter just redraws the \"boxes\" inside the array, it doesn't rearrange the data for a different order of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from skimage import transform\n",
    "from skimage import color\n",
    "\n",
    "class MoviePosterDataset(Dataset):\n",
    "    def __init__(self, csvfile, imagedir, device=device):\n",
    "        self.posterlist = pd.read_csv(csvfile)\n",
    "        self.imagedir = imagedir\n",
    "        \n",
    "        imageids = list(self.posterlist[\"Id\"])\n",
    "        imagefiles = [\"{}/{}.jpg\".format(self.imagedir, x) for x in imageids]\n",
    "        images = [np.array(io.imread(x)) for x in imagefiles]\n",
    "        images = np.array([color.gray2rgb(x) if len(x.shape) < 3 else x for x in images])\n",
    "        \n",
    "        truths = self.posterlist[self.posterlist.columns[2:]]\n",
    "        self.truths = torch.Tensor(truths.to_numpy())\n",
    "    \n",
    "        tns = torch.from_numpy(images)\n",
    "        self.images = tns.permute(0, 3, 1, 2)\n",
    "        \n",
    "        if device != \"cpu\":\n",
    "            self.device = torch.device(device)\n",
    "            self.images = self.images.to(self.device)\n",
    "            self.truths = self.truths.to(self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.posterlist)\n",
    "    \n",
    "    def __getitem__(self, idx):            \n",
    "        truths = self.truths[idx]\n",
    "        images = self.images[idx]\n",
    "        \n",
    "        return images, truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the `Tensor` on the CPU because of memory limitations created by sticking to one GPU (which only has 10GB of space to itself).  We will move it to the GPU in batches instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading the dataset.\n"
     ]
    }
   ],
   "source": [
    "mpd = MoviePosterDataset(\"Multi_Label_dataset/train.csv\", \n",
    "                         \"Multi_Label_dataset/ImageSmaller\", device=device)\n",
    "print(\"Finished loading the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "Defining a model with convolutional layers for images is technically a lot easier than defining an RNN-based model for human language.  The `Conv2d` layer automatically moves a 5x5 filter across the entire image, no effort required to manage padding and packing and unpacking and sequence issues. We do have to flatten the output of the `MaxPool2d` layer to feed it to the subsequent `Linear` layers.  The output of the model comes from `Sigmoid` over the number of classes, so that we have a binary classification for the 25 separate movie labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosterClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Conv2d(3,3,5,padding=2)\n",
    "        self.maxpool = nn.MaxPool2d(5,padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear0 = nn.Linear(3*90*60, 3*90*60)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear1 = nn.Linear(3*90*60, 25)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.conv2d(x)\n",
    "        output = self.maxpool(output)\n",
    "        output = output.view(-1, 3*90*60)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear0(output)\n",
    "        output = self.dropout1(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.linear1(output)\n",
    "        output = self.dropout2(output)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange the data for the model\n",
    "\n",
    "We're going to do a 60/40 train/test split using PyTorch's own samplers and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7254"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalindices = list(range(len(mpd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.shuffle(totalindices)\n",
    "splitindex = math.floor(len(mpd)*0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4352"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part #1: validation data (4 points)\n",
    "\n",
    "Adjust the code in the notebook to give a 60/20/20 train/validation/testing split of the data. Testing data is split 50/50 into test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingindices = totalindices[:splitindex]\n",
    "testingindices_ = totalindices[splitindex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitindex2 = math.floor(len(testingindices_)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitindex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingindices = testingindices_[:splitindex2]\n",
    "validationindices = testingindices_[splitindex2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingsampler = torch.utils.data.SubsetRandomSampler(trainingindices)\n",
    "testingsampler = torch.utils.data.SubsetRandomSampler(testingindices)\n",
    "validationsampler = torch.utils.data.SubsetRandomSampler(validationindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4352, 1451, 1451)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainingsampler), len(testingsampler),len(validationsampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl = torch.utils.data.DataLoader(mpd, batch_size=batches, \n",
    "                                      sampler=trainingsampler, pin_memory=False)\n",
    "valdl = torch.utils.data.DataLoader(mpd, batch_size=batches, sampler=validationsampler, pin_memory=False)\n",
    "\n",
    "testdl = torch.utils.data.DataLoader(mpd, sampler=testingsampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jackard Index\n",
    "Should be of shape [batch_size, image_height, image_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(prediction, ground_truth):\n",
    "    union = prediction + ground_truth\n",
    "    union[union == 2] = 1\n",
    "    intersection = prediction * ground_truth\n",
    "    union = union.sum(axis=(1, 2))\n",
    "    intersection = intersection.sum(axis=(1, 2))\n",
    "    ji_nonezero_union = intersection[union != 0] / union[union != 0]\n",
    "    ji = ji = torch.zeros(intersection.shape)\n",
    "    if device != \"cpu\":\n",
    "        ji = ji.cuda()\n",
    "    ji[union != 0] = ji_nonezero_union\n",
    "    return ji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write and run the training loop\n",
    "\n",
    "We use the training loop to send the data to the GPU, batch by batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric = \"jaccard\"\n",
    "evaluation_metric = \"BCE\"\n",
    "\n",
    "model_name = \"image_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "def train(train_dataloader, val_dataloader, epochs=3):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = PosterClassifier()\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        train_batches = 0     \n",
    "        for c, data in enumerate(train_dataloader):\n",
    "            images, truth = data\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images.float().to(device))\n",
    "            if evaluation_metric != \"jaccard\":\n",
    "                loss = criterion(output, truth.to(device))\n",
    "            train_loss += loss\n",
    "            train_batches += 1.0\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        best_val_loss = 0\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        model.eval()\n",
    "        for c, data in enumerate(val_dataloader):\n",
    "            images, truth = data\n",
    "            output = model(images.float().to(device))\n",
    "            with torch.no_grad():\n",
    "                if evaluation_metric != \"jaccard\":\n",
    "                    loss = criterion(output, truth.to(device))           \n",
    "            val_loss += loss\n",
    "            val_batches += 1.0\n",
    "            \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(\"Best loss so far!\")\n",
    "            # save best model\n",
    "            #torch.save(model.state_dict(), model_name)\n",
    "            torch.save(model, model_name)\n",
    "    \n",
    "        #print(\"In epoch {}, training loss = {}\".format(epoch, train_loss/train_batches))        \n",
    "        #print(\"In epoch {}, validation loss = {}\".format(epoch, val_loss/val_batches))\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss/train_batches:.3f}')\n",
    "        print(f'\\t Validation Loss: {val_loss/val_batches:.3f}')\n",
    "        \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.687%\n",
      "\t Val. Loss: 0.833%\n",
      "Epoch: 02 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.737%\n",
      "\t Val. Loss: 0.768%\n",
      "Epoch: 03 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.809%\n",
      "\t Val. Loss: 0.807%\n",
      "Epoch: 04 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.821%\n",
      "\t Val. Loss: 0.835%\n",
      "Epoch: 05 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.824%\n",
      "\t Val. Loss: 0.781%\n",
      "Epoch: 06 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.833%\n",
      "\t Val. Loss: 0.830%\n",
      "Epoch: 07 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.957%\n",
      "\t Val. Loss: 0.999%\n",
      "Epoch: 08 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 1.194%\n",
      "\t Val. Loss: 1.261%\n",
      "Epoch: 09 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 1.270%\n",
      "\t Val. Loss: 1.391%\n",
      "Epoch: 10 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 1.434%\n",
      "\t Val. Loss: 1.334%\n"
     ]
    }
   ],
   "source": [
    "trained = train(traindl,valdl, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite bad---the loss only gets worse. We need to make adjustments to the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write and run a testing routine\n",
    "\n",
    "For memory purposes, we're keeping the testing data on the CPU memory still.  We have to put the model into evaluation mode with `model.eval`, which turns off the dropout and other regularization useful in training---we want the test to represent a deterministic result of the trained model. We also test a single epoch as one big batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = torch.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    sumloss = 0\n",
    "    items = 0\n",
    "    for c, data in enumerate(dataloader):\n",
    "        images, truth = data\n",
    "        output = model(images.float())\n",
    "        loss = criterion(output, truth)\n",
    "        sumloss += loss\n",
    "        items += 1.0\n",
    "    print(\"Loss on test data = {}\".format(sumloss/items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(trained, testdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is not quite the right way to evaluate a model like this.  However, it is \"encouraging\" that the loss on the test data is not wholly out of line from the loss in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
