{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 6 - movie poster classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label (meaning that there is more than one \"true\" label) classification of movie poster images by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from skimage import io\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We use a PyTorch `Dataset` to represent the data, which means we must implement `__init__`, `__len__` and `__getitem__`.  For efficiency's sake, we ideally want to load the image data (movie posters and their genre classifications) and represent them as a `Tensor` in memory.\n",
    "\n",
    "The movie posters had to be converted to images of the same size and colour channels.  The resizing can be done inside Python but is slow, so they were converted on-disk using command-line tools. The colour channels are more efficient than resizing in Python, so that was done in Python.\n",
    "\n",
    "Note that we need to permute the dimensions of the `Tensor` we create from `skimage` NumPy arrays. The latter represent the colour channels (three, for red-green-blue) as the innermost dimension (the fourth dimension, or dimension 3, meaning that pixels are represented as an array of three colour values).  The convolutional layers in PyTorch require the colour channel to be the second dimension (after the batch dimension, meaning that the image is represented as three overlapping images, one for each colour, where each pixel is a single value).  You need to use `Tensor.permute` not `Tensor.view`---the latter just redraws the \"boxes\" inside the array, it doesn't rearrange the data for a different order of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 23\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from skimage import transform\n",
    "from skimage import color\n",
    "\n",
    "class MoviePosterDataset(Dataset):\n",
    "    def __init__(self, csvfile, imagedir, device=device):\n",
    "        self.posterlist = pd.read_csv(csvfile)\n",
    "        self.imagedir = imagedir\n",
    "        \n",
    "        imageids = list(self.posterlist[\"Id\"])\n",
    "        imagefiles = [\"{}/{}.jpg\".format(self.imagedir, x) for x in imageids]\n",
    "        images = [np.array(io.imread(x)) for x in imagefiles]\n",
    "        images = np.array([color.gray2rgb(x) if len(x.shape) < 3 else x for x in images])\n",
    "        \n",
    "        truths = self.posterlist[self.posterlist.columns[2:]]\n",
    "        self.truths = torch.Tensor(truths.to_numpy())\n",
    "    \n",
    "        tns = torch.from_numpy(images)\n",
    "        self.images = tns.permute(0, 3, 1, 2)\n",
    "        \n",
    "        if device != \"cpu\":\n",
    "            self.device = torch.device(device)\n",
    "            self.images = self.images.to(self.device)\n",
    "            self.truths = self.truths.to(self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.posterlist)\n",
    "    \n",
    "    def __getitem__(self, idx):            \n",
    "        truths = self.truths[idx]\n",
    "        images = self.images[idx]\n",
    "        \n",
    "        return images, truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the `Tensor` on the CPU because of memory limitations created by sticking to one GPU (which only has 10GB of space to itself).  We will move it to the GPU in batches instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading the dataset.\n"
     ]
    }
   ],
   "source": [
    "mpd = MoviePosterDataset(\"Multi_Label_dataset/train.csv\", \n",
    "                         \"Multi_Label_dataset/ImageSmaller\", device=\"cpu\")\n",
    "print(\"Finished loading the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7254 poster images and all the images have been converted to a shape of (3, 300, 450). There are 25 movie labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7254, 3, 300, 450]), torch.Size([7254, 25]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpd.images.shape, mpd.truths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "Defining a model with convolutional layers for images is technically a lot easier than defining an RNN-based model for human language.  The `Conv2d` layer automatically moves a 5x5 filter across the entire image, no effort required to manage padding and packing and unpacking and sequence issues. We do have to flatten the output of the `MaxPool2d` layer to feed it to the subsequent `Linear` layers.  The output of the model comes from `Sigmoid` over the number of classes, so that we have a binary classification for the 25 separate movie labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosterClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "    \n",
    "        # padding should be smaller than half the kernel size\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "        # layer 1\n",
    "        self.conv1  = nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5, stride=1, padding=2)\n",
    "        #self.dropout1 = nn.Dropout(dropout)\n",
    "        #self.maxpool1 = nn.MaxPool2d(kernel_size=5,padding=2)\n",
    "        self.bnm1 = nn.BatchNorm2d(16)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        #self.dropout2 = nn.Dropout(dropout)\n",
    "        self.bnm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        #layer 3\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        #self.dropout3 = nn.Dropout(dropout)\n",
    "        self.bnm3 = nn.BatchNorm2d(64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        #self.linear0 = nn.Linear(3*90*60, 3*90*60)\n",
    "        #self.linear0 = nn.Linear(16*90*60, 16*90*60)\n",
    "        self.linear0 = nn.Linear((64*37*56), 3000)\n",
    "        #self.dropout1 = nn.Dropout(dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "        #self.linear1 = nn.Linear(16*90*60, 25)        \n",
    "        self.linear1 = nn.Linear(3000, 25) # 25 output classes\n",
    "        #self.dropout2 = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # conv -> relu -> maxpool        \n",
    "        #Computes the activation of the first convolution\n",
    "        # X is batch size\n",
    "        \n",
    "        #Layer 1\n",
    "        #Size changes from [X, 3, 300, 450] to [X, 16, 300, 450]\n",
    "        output = self.conv1(input)\n",
    "        output = self.bnm1(output)\n",
    "        output = self.relu(output)\n",
    "        #output = self.dropout(output) \n",
    "        #print(\"Conv1\", output.size())\n",
    "        #Size changes from [X, 16, 300, 450] to [X, 16, 150, 225]\n",
    "        output = self.maxpool1(output)\n",
    "        #print(\"Pool1\", output.size())\n",
    "        \n",
    "        #Layer 2\n",
    "        #Size changes from [X, 16, 150, 225] to [X, 32, 150, 225]\n",
    "        output = self.conv2(output)\n",
    "        output = self.bnm2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output)   \n",
    "        #print(\"Conv2\", output.size())\n",
    "        #Size changes from [X, 32, 75, 112] to [X, 32, 75, 112]\n",
    "        output = self.maxpool2(output)\n",
    "        #print(\"Pool2\", output.size())\n",
    "        \n",
    "        #Layer 3\n",
    "        #Size changes from [X, 32, 75, 112] to [X, 64, 75, 112]\n",
    "        output = self.conv3(output)\n",
    "        output = self.bnm3(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout(output) \n",
    "        #print(\"Conv3\", output.size())\n",
    "        #Size changes from [X, 64, 75, 112) to [X, 64, 37, 56]\n",
    "        output = self.maxpool3(output)\n",
    "        #print(\"Pool3\", output.size())\n",
    "        \n",
    "        #output = output.view(-1, 3*90*60)\n",
    "        #output = output.view(-1, 16*90*60)\n",
    "        output = output.view(-1, 64*37*56) # output size of final pooling layer\n",
    "        output = self.relu(output)\n",
    "        output = self.linear0(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.linear1(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange the data for the model\n",
    "\n",
    "We're going to do a 60/40 train/test split using PyTorch's own samplers and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7254"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalindices = list(range(len(mpd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "random.shuffle(totalindices)\n",
    "splitindex = math.floor(len(mpd)*0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4352"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitindex"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# sample set used for testing part of the dataset\n",
    "#sample = len(mpd)//4\n",
    "#totalindices = list(range((sample)))\n",
    "#random.shuffle(totalindices)\n",
    "#splitindex = math.floor((sample)*0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part #1: validation data (4 points)\n",
    "\n",
    "Adjust the code in the notebook to give a 60/20/20 train/validation/testing split of the data. Testing data is split 50/50 into test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingindices = totalindices[:splitindex]\n",
    "testingindices_ = totalindices[splitindex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitindex2 = math.floor(len(testingindices_)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitindex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingindices = testingindices_[:splitindex2]\n",
    "validationindices = testingindices_[splitindex2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingsampler = torch.utils.data.SubsetRandomSampler(trainingindices)\n",
    "testingsampler = torch.utils.data.SubsetRandomSampler(testingindices)\n",
    "validationsampler = torch.utils.data.SubsetRandomSampler(validationindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4352\n",
      "Number of validation examples: 1451\n",
      "Number of testing examples: 1451\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(trainingsampler)}')\n",
    "print(f'Number of validation examples: {len(validationsampler)}')\n",
    "print(f'Number of testing examples: {len(testingsampler)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl = torch.utils.data.DataLoader(mpd, batch_size=batches, \n",
    "                                      sampler=trainingsampler, pin_memory=False)\n",
    "valdl = torch.utils.data.DataLoader(mpd, batch_size=batches, sampler=validationsampler, pin_memory=False)\n",
    "testdl = torch.utils.data.DataLoader(mpd, sampler=testingsampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jackard Index\n",
    "The size of the intersection divided by the size of the union of two label sets:\n",
    "A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "\n",
    "CNN for image identification applications apply the Jaccard Index measurements as a way of conceptualizing accuracy of object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(output, truth):\n",
    "    jaccard_loss = 0.0\n",
    "    intersection = 0.0\n",
    "    union = 0.0\n",
    "    for i in range(len(truth)):\n",
    "        for tru,out in zip(truth[i],output[i]):\n",
    "            tru = tru.item()\n",
    "            #why not cast bool to float\n",
    "            #out = (out.item()>0.5).float()\n",
    "            out = 1.0 if out.item() > 0.5 else 0.0\n",
    "            if tru == out:\n",
    "               intersection += 1.0\n",
    "            union += 1.0            \n",
    "            jaccard_loss += intersection/union\n",
    "    return jaccard_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def jaccard_numpy(output, truth):\n",
    "    #component1 = np.array([[0,1,1],[0,1,1],[0,1,1]], dtype=bool)\n",
    "    #component2 = np.array([[1,1,0],[1,1,0],[1,1,0]], dtype=bool)\n",
    "    \n",
    "\n",
    "    overlap = output*truth # Logical AND\n",
    "    union = output + truth # Logical OR\n",
    "\n",
    "    IOU = overlap/float(union) # Treats \"True\" as 1,\n",
    "                                           # sums number of Trues\n",
    "                                           # in overlap and union\n",
    "                                           # and divides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write and run the training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"image_model\"\n",
    "model = PosterClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosterClassifier(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bnm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bnm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bnm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (linear0): Linear(in_features=132608, out_features=3000, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (linear1): Linear(in_features=3000, out_features=25, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 397,967,561 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate= 0.001\n",
    "decay=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def train(train_dataloader, val_dataloader, model,epochs=3):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    criterion = criterion.to(device)\n",
    "    best_val_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for c, data in enumerate(train_dataloader):\n",
    "            images, truth = data\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images.float().to(device))\n",
    "            loss = criterion(output, truth.to(device))\n",
    "            train_loss += loss\n",
    "            train_batches += 1.0\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0        \n",
    "        jaccard_loss = 0.0\n",
    "        \n",
    "        model.eval()\n",
    "        for c, data in enumerate(val_dataloader):\n",
    "            images, truth = data\n",
    "            output = model(images.float().to(device))\n",
    "            with torch.no_grad():  \n",
    "                val_loss += criterion(output, truth.to(device))\n",
    "                jaccard_loss += jaccard(output, truth.to(device)) \n",
    "        \n",
    "            val_batches += 1.0\n",
    "            \n",
    "        validation_loss = val_loss/val_batches\n",
    "        if epoch == 0:\n",
    "            best_val_loss = validation_loss        \n",
    "        if validation_loss < best_val_loss:\n",
    "            best_val_loss = validation_loss\n",
    "            print(\"Best loss so far!\")\n",
    "            # save best model\n",
    "            #torch.save(model.state_dict(), model_name)\n",
    "            torch.save(model, model_name)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss/train_batches:.3f}')\n",
    "        print(f'\\t Validation Loss: {validation_loss:.3f}')\n",
    "        print(f'\\t Lowest Loss: {best_val_loss:.3f}')\n",
    "        print(f'\\t Jaccard Index: {jaccard_loss/float(batches*val_batches):.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.294\n",
      "\t Validation Loss: 0.238\n",
      "\t Lowest Loss: 0.238\n",
      "\t Jaccard Index: 22.520\n",
      "Epoch: 02 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.241\n",
      "\t Validation Loss: 0.238\n",
      "\t Lowest Loss: 0.238\n",
      "\t Jaccard Index: 22.522\n",
      "Epoch: 03 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.241\n",
      "\t Validation Loss: 0.238\n",
      "\t Lowest Loss: 0.238\n",
      "\t Jaccard Index: 22.512\n",
      "Epoch: 04 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.241\n",
      "\t Validation Loss: 0.238\n",
      "\t Lowest Loss: 0.238\n",
      "\t Jaccard Index: 22.576\n",
      "Best loss so far!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type PosterClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.240\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.237\n",
      "\t Jaccard Index: 22.540\n",
      "Best loss so far!\n",
      "Epoch: 06 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.240\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.237\n",
      "\t Jaccard Index: 22.574\n",
      "Epoch: 07 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.240\n",
      "\t Validation Loss: 0.239\n",
      "\t Lowest Loss: 0.237\n",
      "\t Jaccard Index: 22.553\n",
      "Epoch: 08 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.240\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.237\n",
      "\t Jaccard Index: 22.586\n",
      "Best loss so far!\n",
      "Epoch: 09 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.240\n",
      "\t Validation Loss: 0.235\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.592\n",
      "Epoch: 10 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.236\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.586\n",
      "Epoch: 11 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.545\n",
      "Epoch: 12 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.238\n",
      "\t Validation Loss: 0.236\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.621\n",
      "Epoch: 13 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.565\n",
      "Epoch: 14 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.606\n",
      "Epoch: 15 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.564\n",
      "Epoch: 16 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.612\n",
      "Epoch: 17 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.239\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.529\n",
      "Epoch: 18 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.240\n",
      "\t Validation Loss: 0.236\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.660\n",
      "Epoch: 19 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.236\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.622\n",
      "Epoch: 20 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.590\n",
      "Epoch: 21 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.236\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.627\n",
      "Epoch: 22 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.237\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.599\n",
      "Epoch: 23 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.240\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.568\n",
      "Epoch: 24 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.236\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.602\n",
      "Epoch: 25 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.240\n",
      "\t Validation Loss: 0.236\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.585\n",
      "Epoch: 26 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.240\n",
      "\t Validation Loss: 0.236\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.654\n",
      "Epoch: 27 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.240\n",
      "\t Validation Loss: 0.238\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.569\n",
      "Best loss so far!\n",
      "Epoch: 28 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.238\n",
      "\t Validation Loss: 0.235\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.595\n",
      "Epoch: 29 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.238\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.589\n",
      "Epoch: 30 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.239\n",
      "\t Validation Loss: 0.238\n",
      "\t Lowest Loss: 0.235\n",
      "\t Jaccard Index: 22.569\n"
     ]
    }
   ],
   "source": [
    "trained = train(traindl,valdl, model,epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write and run a testing routine\n",
    "\n",
    "For memory purposes, we're keeping the testing data on the CPU memory still.  We have to put the model into evaluation mode with `model.eval`, which turns off the dropout and other regularization useful in training---we want the test to represent a deterministic result of the trained model. We also test a single epoch as one big batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = torch.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    sumloss = 0\n",
    "    items = 0\n",
    "    for c, data in enumerate(dataloader):\n",
    "        images, truth = data\n",
    "        output = model(images.float())\n",
    "        loss = criterion(output, truth)\n",
    "        sumloss += loss\n",
    "        items += 1.0\n",
    "    print(\"Loss on test data = {}\".format(sumloss/items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(trained, testdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is not quite the right way to evaluate a model like this.  However, it is \"encouraging\" that the loss on the test data is not wholly out of line from the loss in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
